{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'nlp_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Hwx0ajjDny"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gKY4wCIRiwhF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tauru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QDoiMEdmjUji"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UafqBqxIjUkt"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)  # str to list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kKUmetoTkSXB"
   },
   "outputs": [],
   "source": [
    "train = read_data(base_path+'/train.tsv')\n",
    "validation = read_data(base_path+'/validation.tsv')\n",
    "test = pd.read_csv(base_path+'/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Efa8kLpEklM0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to draw a stacked dotplot in R?</td>\n",
       "      <td>[r]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mysql select all records where a datetime fiel...</td>\n",
       "      <td>[php, mysql]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to terminate windows phone 8.1 app</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get current time in a specific country via jquery</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Configuring Tomcat to Use SSL</td>\n",
       "      <td>[java]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  tags\n",
       "0                How to draw a stacked dotplot in R?                   [r]\n",
       "1  mysql select all records where a datetime fiel...          [php, mysql]\n",
       "2             How to terminate windows phone 8.1 app                  [c#]\n",
       "3  get current time in a specific country via jquery  [javascript, jquery]\n",
       "4                      Configuring Tomcat to Use SSL                [java]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bt5vsnYLlRIw"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lljwMAzlu6T"
   },
   "source": [
    "#### Task 1 (TextPrepare).\n",
    "#### Implement the function text_prepare following the instructions.text_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mG7ItnqKl4el"
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text \n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    # delete stopwords from \n",
    "    text = ' '.join(s for s in text.split() if s not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M1RXkSb2oyNc"
   },
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uKU3N9EPo0iY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj4vHyWgJEYR"
   },
   "source": [
    "#### Task 2 (WordsTagsCount)\n",
    "#### Find 3 most popular tags and 3 most popular words in the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MvAxFp3EJM7j"
   },
   "outputs": [],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "#words_counts are dictionaries like {'some_word_or_tag': frequency}. \n",
    "\n",
    "for x in X_train:\n",
    "    x = text_prepare(x)\n",
    "    x_list = x.split()\n",
    "    for word in x_list:\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 0\n",
    "        else:\n",
    "            words_counts[word] += 1\n",
    "for y in y_train:\n",
    "    for tag in y:\n",
    "        if tag not in tags_counts:\n",
    "            tags_counts[tag] = 0\n",
    "        else:\n",
    "            tags_counts[tag] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SlkjuMy7KWMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('javascript', 19077), ('c#', 19076), ('java', 18660)]\n",
      "[('using', 8277), ('php', 5613), ('java', 5500)]\n"
     ]
    }
   ],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(most_common_tags)\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWg9KBJjKuGs"
   },
   "source": [
    "## Transforming text to a vector\n",
    "#### Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8uUt3WBLe67"
   },
   "source": [
    "### Bag of words\n",
    "#### One of the well-known approaches is a bag-of-words representation. To create this transformation, follow the steps:\n",
    "\n",
    "\n",
    "\n",
    "1.   Find N most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2.  For each title in the corpora create a zero vector with the dimension equals to N.\n",
    "3.  For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YXbn3lYsK17l"
   },
   "outputs": [],
   "source": [
    "# Implement the described encoding in the function my_bag_of_words with the size of the dictionary equals to 5000. \n",
    "DICT_SIZE = 5000\n",
    "WORDS_TO_INDEX = {word: index for word, index in zip([key for key, value in sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]], [i for i in range(DICT_SIZE)])}\n",
    "INDEX_TO_WORDS = {index: word for word, index in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for x in text.split(' '):\n",
    "        if x in words_to_index.keys():\n",
    "            result_vector[words_to_index[x]] += 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JwDDQnqQQbkf"
   },
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PRmJUlupQeJ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jJJir73DjUud"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qe1GzmpIjeqF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxp4pse_X80j"
   },
   "source": [
    "### Task 3 (BagOfWords). \n",
    "#### For the 11th row in X_train_mybag find how many non-zero elements it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r_LiHHEyBTxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i in X_train_mybag.toarray()[10]:\n",
    "    if i != 0:\n",
    "        num += 1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySPI9pH9CZwc"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "91RcW2R3CbaN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sPO9wpHTZ8Zf"
   },
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test â€” samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words = STOPWORDS, max_features = 5000, token_pattern = r\"([a-z0-9#+_]{1,})\", min_df = 0.00007 )   \n",
    "    \n",
    "    X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val = tfidf_vectorizer.transform(X_val)\n",
    "    X_test = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cdjjPEjNFQdh"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZcUp-qXghsq"
   },
   "source": [
    "check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m6xZQKYfHBoA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c++\n",
      "c#\n"
     ]
    }
   ],
   "source": [
    "if 'c++' in tfidf_vocab.keys():\n",
    "    print('c++')\n",
    "if 'c#' in tfidf_vocab.keys():\n",
    "    print('c#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSeNTUGGgmhh"
   },
   "source": [
    "If you can't find it, we need to understand how did it happen that we lost them? It happened during the built-in tokenization of TfidfVectorizer. Luckily, we can influence on this process. Get back to the function above and use '(\\S+)' regexp as a token_pattern in the constructor of the vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ51T4K6hASZ"
   },
   "source": [
    "## MultiLabel classifier\n",
    "\n",
    "\n",
    "*   compare the quality of the bag-of-words and TF-IDF approaches \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y_train_matrix = MultiLabelBinarizer().fit_transform(y_train)\n",
    "y_val_matrix = MultiLabelBinarizer().fit_transform(y_val)\n",
    "\n",
    "classifier_mybag = OneVsRestClassifier(LogisticRegression(penalty = 'l2', C = 4.0, max_iter=10000)).fit(X_train_mybag, y_train_matrix)\n",
    "classifier_tfidf = OneVsRestClassifier(LogisticRegression(penalty = 'l2', C = 4.0, max_iter=10000)).fit(X_train_tfidf, y_train_matrix)\n",
    "\n",
    "predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdGWrh-hfof"
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "\n",
    "*   accuracy\n",
    "*   F1-score macro/micro\n",
    "* Precision macro/micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "\n",
      "Bag-of-words\n",
      "\n",
      "Accuracy:  3222\n",
      "F1-score macro:  0.21602547872639005\n",
      "F1-score micro:  0.3136007181897394\n",
      "Precision macro:  0.10720629514044663\n",
      "Precision micro:  0.15542153307810525\n",
      "\n",
      "Tfidf\n",
      "\n",
      "Accuracy:  11164\n",
      "F1-score macro:  0.5028549895745037\n",
      "F1-score micro:  0.6802764662733876\n",
      "Precision macro:  0.3432655061405315\n",
      "Precision micro:  0.4924369337510154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
    "    print('F1-score macro: ', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro: ', f1_score(y_test, predicted, average='micro'))\n",
    "    print('Precision macro: ', average_precision_score(y_test, predicted, average='macro'))\n",
    "    print('Precision micro: ', average_precision_score(y_test, predicted, average='micro'))\n",
    "\n",
    "print('LogisticRegression:\\n')\n",
    "print('Bag-of-words\\n')\n",
    "print_evaluation_scores(y_val_matrix, predicted_labels_mybag)\n",
    "print('\\nTfidf\\n')\n",
    "print_evaluation_scores(y_val_matrix, predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS7w0D6Y0GsJ"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### ex: mean(word embeddings) --> MLP\n",
    "#### ex: word embeddings --> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.models import Sequential\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "word_index = tokenizer.word_index\n",
    "data_train = pad_sequences(sequences_train)\n",
    "data_val = pad_sequences(sequences_val)\n",
    "MLP_val = np.c_[data_val, np.array([0 for i in range(data_val.shape[0])]).T]\n",
    "EMBEDDING_DIM = 500\n",
    "\n",
    "sentence_train = [[word for word in text_prepare(words).split()] for words in X_train]\n",
    "sentence_val = [[word for word in text_prepare(words).split()] for words in X_val]\n",
    "w2v_model = Word2Vec(sentence_train, min_count = 1, size = EMBEDDING_DIM, workers = 3, window = 5)\n",
    "embedding_layer = Embedding(w2v_model.wv.vectors.shape[0], w2v_model.wv.vectors.shape[1], weights=[w2v_model.wv.vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tauru\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 500)         15748500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500)               2002000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               50100     \n",
      "=================================================================\n",
      "Total params: 17,800,600\n",
      "Trainable params: 17,800,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(embedding_layer)\n",
    "model_LSTM.add(LSTM(w2v_model.wv.vectors.shape[1]))\n",
    "model_LSTM.add(Dense(y_train_matrix.shape[1], activation = 'softmax'))  \n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tauru\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 428s 4ms/step - loss: 9.8018 - acc: 0.1198\n",
      "30000/30000 [==============================] - 100s 3ms/step\n",
      "accuracy = 0.1152999997138977\n"
     ]
    }
   ],
   "source": [
    "model_LSTM.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['acc'])\n",
    "model_LSTM.fit(data_train, y_train_matrix, epochs = 1, batch_size = 256)\n",
    "loss_LSTM, acc_LSTM = model_LSTM.evaluate(data_val, y_val_matrix)\n",
    "print('accuracy =', acc_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "model_MLP = Sequential()\n",
    "model_MLP.add(Dense(512, activation='relu'))\n",
    "model_MLP.add(Dense(y_train_matrix.shape[1], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100000/100000 [==============================] - 3s 26us/step - loss: 465.8285 - accuracy: 0.0970\n",
      "Epoch 2/5\n",
      "100000/100000 [==============================] - 2s 24us/step - loss: 15.0916 - accuracy: 0.0973\n",
      "Epoch 3/5\n",
      "100000/100000 [==============================] - 3s 26us/step - loss: 15.5843 - accuracy: 0.0972\n",
      "Epoch 4/5\n",
      "100000/100000 [==============================] - 3s 26us/step - loss: 54.8741 - accuracy: 0.0973\n",
      "Epoch 5/5\n",
      "100000/100000 [==============================] - 3s 28us/step - loss: 104.4462 - accuracy: 0.0973\n",
      "30000/30000 [==============================] - 3s 105us/step\n",
      "accuracy = 0.09399999678134918\n"
     ]
    }
   ],
   "source": [
    "model_MLP.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "model_MLP.fit(data_train, y_train_matrix, epochs = 5, batch_size = 256)\n",
    "loss_MLP, acc_MLP = model_MLP.evaluate(MLP_val, y_val_matrix)\n",
    "print('accuracy =', acc_MLP)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp_hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tfenv] *",
   "language": "python",
   "name": "conda-env-tfenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
